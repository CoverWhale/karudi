# ğŸ§  Data Migration DQ Checks

This project implements a complete **ELT + DQ validation pipeline** for verifying data migrations between internal CW systems and Solartis Gateway data.

It processes both **CSV source data** and **JSON migrated data**, performs **structured comparisons**, and generates **visual + Excel-based reports** of mismatches.

---

## ğŸ“ Project Structure

```
data-migration-dq-checks/
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ template_request_files/
â”‚   â”‚   â”œâ”€â”€ find_policy.json
â”‚   â”‚   â”œâ”€â”€ find_quote.json
â”‚   â”‚   â”œâ”€â”€ find_submission.json
â”‚   â”‚   â”œâ”€â”€ get_documents.json
â”‚   â”‚   â”œâ”€â”€ get_notes.json
â”‚   â”‚   â”œâ”€â”€ get_policy.json
â”‚   â”‚   â”œâ”€â”€ get_quote.json
â”‚   â”‚   â””â”€â”€ get_submission.json
â”‚   â””â”€â”€ etl_diagram.png
â”œâ”€â”€ dq_checks/
â”‚   â”œâ”€â”€ dq_table_map.py
â”‚   â”œâ”€â”€ dq_visuals.py
â”‚   â”œâ”€â”€ execute.py
â”‚   â””â”€â”€ execute_new.py
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”œâ”€â”€ extract_drive.py
â”‚   â”‚   â”œâ”€â”€ extract_drive_old.py
â”‚   â”‚   â”œâ”€â”€ extract_solartis_api.py
â”‚   â”‚   â”œâ”€â”€ extract_solartis_api_old.py
â”‚   â”‚   â””â”€â”€ generate submission_failure_log.py
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”œâ”€â”€ csv_load_to_clickhouse.py
â”‚   â”‚   â”œâ”€â”€ json_load_to_clickhouse.py
â”‚   â”‚   â””â”€â”€ tables/
â”‚   â”‚       â”œâ”€â”€ cw/
â”‚   â”‚       â”‚   â”œâ”€â”€ Policy/
â”‚   â”‚       â”‚   â”œâ”€â”€ Quotes/
â”‚   â”‚       â”‚   â””â”€â”€ Wip/
â”‚   â”‚       â””â”€â”€ solartis/
â”‚   â”‚           â”œâ”€â”€ Policy/
â”‚   â”‚           â”œâ”€â”€ Quotes/
â”‚   â”‚           â””â”€â”€ Wip/
â”‚   â””â”€â”€ transform/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ Policy/
â”‚       â”‚   â”œâ”€â”€ Quotes/
â”‚       â”‚   â””â”€â”€ Wip/
â”‚       â””â”€â”€ run_transformation_sql.sh
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ data-migration-dq-checks.json
â”œâ”€â”€ failed_submissions.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_pipeline.sh
â””â”€â”€ script.log

```

---

## âš™ï¸ ELT Overview

![Migration Data Quality Checks](assets/etl_diagram.png)

### Extract
- `.csv` files = CW source data (Quotes, WIP, Policy)
- `.json` files = Solartis migrated payloads (submission, quote, policy)

### Load
- Custom loaders insert both CSV and JSON into **ClickHouse**
- Each model (e.g. `cw_migration_wip`, `solartis_wip_submissions`) has isolated table structure

### Transform
- SQL transformations normalize nested Solartis JSON (e.g. `submissions.sql`, `vehicles.sql`)
- Creates aggregate versions: `taxes_agg`, `fees_agg`

---

## âœ… Data Quality Checks

### Run a DQ comparison:
```bash
python dq_checks/execute_new.py --model WIP --date 2025-03-15
```

This will:
- Compare each CW table to its Solartis counterpart
- Calculate:
  - Row-level match %
  - Column-level mismatch counts
  - False mismatch (e.g. CW value is null, Solartis is not)
  - Financial field aggregations
- Generate:
  - Excel report with vertical diff breakdowns
  - CSV exports for each tab

---

## ğŸ“Š Visual Insights

Once the DQ run is complete, you can generate visualizations from the exported CSVs:

```bash
python dq_checks/dq_visuals.py
```

This will show:
- Column match % by table
- Row vs column match ratio
- Column mismatch heatmaps
- False mismatch bar charts

---

## ğŸ“¦ Requirements

```bash
pip install -r requirements.txt
```

Key packages:
- `pandas`, `polars`
- `matplotlib`, `seaborn`
- `openpyxl`
- `clickhouse-connect`

---

## ğŸ§¼ Git Hygiene

`.gitignore` ensures:
- No `.csv`, `.json`, or `.log` files are committed
- All DQ outputs and test results stay local

---

## ğŸ› ï¸ Future Enhancements

- Slack/email alerts on critical mismatches
- Streamlit dashboard integration
- Auto-publish visual reports
